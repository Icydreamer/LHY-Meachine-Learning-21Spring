# 机器学习

## Introduction

### 机器学习

机器学习 = 让机器找函数

+ 语音识别：$ f(x) = $ "How are you" 

### 深度学习

使用类神经网络，来找到函数

### 输入输出

输入多种多样：

+ 向量
+ 矩阵（图像）
+ 序列（语音，文字）

输出：

+ 数值（regression）
+ 类别（classification）
+ 文本、图像

### 教机器找函数

#### Supervised Learning

监督学习

lecture 1-5

对收集的数据加labels

#### Self-Supervised Learning

自监督学习

lecture 7

1. pre-train
2. downstream tasks

Pre-trained Model（Foundation Model）

BERT： 340 parameters

Downstream Tasks

#### Generative Adversarial Network

生成式对抗网络

lecture 6

无需成对的样本数据

#### Reinforcement Learning

强化学习

lecture 12

人类无法自己标注资料

#### Anomaly Detection

异常检测

lecture 8

具备回答：“我不知道”的能力

#### Explainable AI

可解释

lecture 9

知道为什么模型是这样的？

#### Model Attack

lecture 10

对已有的模型加噪点，为什么得到不一样的结论？

#### Domain Adaptation

lecture 11



#### Network Compression

模型压缩

lecture 13

#### Life-long Learning

lecture 14



#### Meta Learning

learn how to learning

lecture 15

让机器自己学习算法





## 第一节

### 机器学习

机器学习 = 让机器找函数

+ 语音识别：$ f(x) = $ "How are you" 

### 深度学习

使用类神经网络，来找到函数

### 不同种的函数

三大任务

+ 数值（regression）
+ 类别（classification）
  + 给定多种options， 函数输出正确的一种
+ Structured Learning
  + 创建结构化的数据

### ML Framework

Step 1: function with unknown

Step 2: define loss from training data

Step 3: optimization

### Step1: How to find a function?

1. Function with Unknown Parameters

   1. 写出带有未知数的式子，猜测函数
   2. $y=b+wx_1$
   3. y：模型（Model），$x_1$： feature，w： weight，b： bias
   4. Domain knowledge

2. Define Loss from Training Data

   1. Loss is a function of parameters L(b,w)
   2. Loss: how good a set of values is. 评价函数好坏
   3. Loss： $L=\frac1N \sum(e_n)$
   4. mean absolute error ( MAE) 
   5. mean square error ( MSE) 

3. Optimization（最佳化）

   1. $w*,b*=arg min L$
   2. Gradient Descent
      1. (Randomly) Pick an initial value w^0
      2. Compute $\frac {\partial L}{\partial w} = w^0$
         1. Negative = increase
         2. activate =  decrease
      3. $\eta \frac {\partial L}{\partial w} = w^0$ $\eta$: learning rate（hyper parameters）
      4. Update w iteratively
      5. local minima？global minima 

   Linear Model

   Linear models have severe limitation. ->Model Bias

   

Piecewise Linear Curves

分段函数



Beyond Piecewise Linear

光滑函数，取点，分段，当作分段函数



用分段函数取逼近任何曲线
$$
y=c\frac 1{1+e^{-(b+wx_1)}}=c\space sigmoid(b+wx_1)
$$
Sigmoid Function S型函数

Hard Sigmoid

 使用多个S型函数，构建目标函数
$$
y=b+\sum_i c_isigmoid(b_i+w_ix_1)
$$
New Model: More Features
$$
y=b+wx_1 \to y=b+\sum_i c_i sigmoid (b_i+w_ix_1)
$$

$$
y=b+\sum_j w_jx_j \to y=b+\sum_i c_i sigmoid(b_i+\sum_j w_{ij}x_j)
$$

例
$$
y=b+\sum_i c_i sigmoid(b_i+\sum_j w_{ij}x_j)\newline
r_1=b_1+w_{11}x_1+w_{12}x_2+w_{13}x_3\newline
r_2=b_2+w_{21}x_1+w_{22}x_2+w_{23}x_3\newline
r_3=b_3+w_{31}x_1+w_{32}x_2+w_{33}x_3\newline 
\to \newline
\begin{bmatrix}
r_1\\
r_2\\
r_3\\
\end{bmatrix}
=
\begin{bmatrix}
b_1\\
b_2\\
b_3\\
\end{bmatrix}
+
\begin{bmatrix}
w_{12} & w_{12} & w_{13}\\
w_{21} & w_{22} & w_{23}\\
w_{31} & w_{32} & w_{33}\\
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
\end{bmatrix}
\to \\
\overrightarrow {r}=\overrightarrow {b}+W  \space \overrightarrow {x}\\
a_1=sigmoid(r_1)=\frac 1{1+e^{-r_1}}\\
\overrightarrow {a}=\sigma (r)\\
y=b+\overrightarrow {c}\space ^T \space \overrightarrow {a}=b+\overrightarrow {c}\space ^T \space \sigma \space(\overrightarrow {b}+W\overrightarrow {x})\\
\theta
$$
$\theta$是所有未知参数形成的一个向量

### Step 2

Loss function: $L(\theta)$

Loss决定模型的好坏程度

$\theta ^* = arg \space min_{\theta} L$
$$
\overrightarrow g = L(\theta^0)
$$

$$
ReLu
$$



Neuron

Neuron Network

Deep Learning

Deep的理由？
